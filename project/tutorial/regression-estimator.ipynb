{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "from tensorflow.python.feature_column import feature_column_v2\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'reg-model-01'\n",
    "\n",
    "TRAIN_DATA_FILE = 'data/train-data.csv'\n",
    "VALID_DATA_FILE = 'data/valid-data.csv'\n",
    "TEST_DATA_FILE = 'data/test-data.csv'\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "PROCESS_FEATURES = True\n",
    "MULTI_THREADING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['key','x','y','alpha','beta','target']\n",
    "HEADER_DEFAULTS = [[0], [0.0], [0.0], ['NA'], ['NA'], [0.0]]\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE, names=HEADER, skiprows=0)\n",
    "valid_df = pd.read_csv(VALID_DATA_FILE, names=HEADER, skiprows=0)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE, names=HEADER, skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17262</td>\n",
       "      <td>0.893902</td>\n",
       "      <td>0.040267</td>\n",
       "      <td>ax02</td>\n",
       "      <td>bx02</td>\n",
       "      <td>-12.314443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4554</td>\n",
       "      <td>0.148486</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>ax01</td>\n",
       "      <td>bx01</td>\n",
       "      <td>3.269937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19003</td>\n",
       "      <td>0.156807</td>\n",
       "      <td>-0.953493</td>\n",
       "      <td>ax02</td>\n",
       "      <td>bx02</td>\n",
       "      <td>39.556516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15196</td>\n",
       "      <td>0.374318</td>\n",
       "      <td>0.306750</td>\n",
       "      <td>ax02</td>\n",
       "      <td>bx02</td>\n",
       "      <td>-1.011721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9502</td>\n",
       "      <td>0.827361</td>\n",
       "      <td>0.305439</td>\n",
       "      <td>ax02</td>\n",
       "      <td>bx01</td>\n",
       "      <td>6.051278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     key         x         y alpha  beta     target\n",
       "0  17262  0.893902  0.040267  ax02  bx02 -12.314443\n",
       "1   4554  0.148486  0.015758  ax01  bx01   3.269937\n",
       "2  19003  0.156807 -0.953493  ax02  bx02  39.556516\n",
       "3  15196  0.374318  0.306750  ax02  bx02  -1.011721\n",
       "4   9502  0.827361  0.305439  ax02  bx01   6.051278"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 3000, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['key', 'x', 'y', 'alpha', 'beta', 'target']\n",
      "Numeric Features: ['x', 'y']\n",
      "Categorical Features: ['alpha', 'beta']\n",
      "Target: target\n",
      "Unused Features: ['key']\n"
     ]
    }
   ],
   "source": [
    "NUMERIC_FEATURE_NAMES = ['x', 'y']\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'alpha':['ax01', 'ax02'], 'beta': ['bx01', 'bx02']}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "TARGET_NAME = 'target'\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME})\n",
    "\n",
    "print(\"Header: {}\".format(HEADER))\n",
    "print(\"Numeric Features: {}\".format(NUMERIC_FEATURE_NAMES))\n",
    "print(\"Categorical Features: {}\".format(CATEGORICAL_FEATURE_NAMES))\n",
    "print(\"Target: {}\".format(TARGET_NAME))\n",
    "print(\"Unused Features: {}\".format(UNUSED_FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(dataset_df):\n",
    "    dataset_df['x_2'] = np.square(dataset_df['x'])\n",
    "    dataset_df['y_2'] = np.square(dataset_df['y'])\n",
    "    dataset_df['xy'] = dataset_df['x'] * dataset_df['y']\n",
    "    dataset_df['dist_xy'] = np.sqrt(np.square(dataset_df['x'] - dataset_df['y']))\n",
    "    return dataset_df\n",
    "\n",
    "def generate_pandas_input_fn(file_name, mode=tf.estimator.ModeKeys.EVAL,\n",
    "                            skip_header_lines=0,\n",
    "                            num_epochs=1,\n",
    "                            batch_size=100):\n",
    "    df_dataset = pd.read_csv(file_name, names=HEADER, skiprows=skip_header_lines)\n",
    "    \n",
    "    x = df_dataset[FEATURE_NAMES].copy()\n",
    "    if PROCESS_FEATURES:\n",
    "        x = process_dataframe(x)\n",
    "    y = df_dataset[TARGET_NAME]\n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    num_threads=1\n",
    "    \n",
    "    if MULTI_THREADING:\n",
    "        num_threads = multiprocessing.cpu_count()\n",
    "        num_epochs = int(num_epochs / num_threads) if mode == tf.estimator.ModeKeys.TRAIN else num_epochs\n",
    "        \n",
    "    pandas_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        target_column=TARGET_NAME\n",
    "    )\n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file: {}\".format(file_name))\n",
    "    print(\"Dataset size: {}\".format(len(df_dataset)))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Thread Count: {}\".format(num_threads))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return pandas_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/train-data.csv\n",
      "Dataset size: 12000\n",
      "Batch size: 100\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Feature read from DataFrame: ['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']\n",
      "Target read from DataFrame: Tensor(\"fifo_queue_DequeueUpTo:9\", shape=(?,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "features, target = generate_pandas_input_fn(file_name=TRAIN_DATA_FILE)()\n",
    "print(\"Feature read from DataFrame: {}\".format(list(features.keys())))\n",
    "print(\"Target read from DataFrame: {}\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'x': NumericColumn(key='x', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y': NumericColumn(key='y', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'x_2': NumericColumn(key='x_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y_2': NumericColumn(key='y_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'xy': NumericColumn(key='xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'dist_xy': NumericColumn(key='dist_xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'alpha': VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'beta': VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'alpha_X_beta': CrossedColumn(keys=(VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=4, hash_key=None)}\n"
     ]
    }
   ],
   "source": [
    "def get_feature_columns():\n",
    "    all_numeric_feature_names = NUMERIC_FEATURE_NAMES\n",
    "    CONSTRUCTED_NUMERIC_FEATURES_NAMES = ['x_2', 'y_2', 'xy', 'dist_xy']\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        all_numeric_feature_names += CONSTRUCTED_NUMERIC_FEATURES_NAMES\n",
    "    numeric_columns = {feature_name: tf.feature_column.numeric_column(feature_name)\n",
    "                      for feature_name in all_numeric_feature_names}\n",
    "    categorical_column_with_vocabulary = \\\n",
    "    {item[0]: tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1])\n",
    "    for item in CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()}\n",
    "#     print(categorical_column_with_vocabulary)\n",
    "    \n",
    "    feature_columns = {}\n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "    if categorical_column_with_vocabulary is not None:\n",
    "        feature_columns.update(categorical_column_with_vocabulary)\n",
    "    feature_columns['alpha_X_beta'] = tf.feature_column.crossed_column(\n",
    "        [feature_columns['alpha'], feature_columns['beta']], 4\n",
    "    )\n",
    "    return feature_columns\n",
    "\n",
    "feature_columns = get_feature_columns()\n",
    "print('Feature Columns: {}'.format(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "    feature_columns = list(get_feature_columns().values())\n",
    "    dense_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column_v2.NumericColumn),\n",
    "              feature_columns)\n",
    "    )\n",
    "    categorical_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column_v2.VocabularyListCategoricalColumn) |\n",
    "              isinstance(column, feature_column_v2.BucketizedColumn),\n",
    "              feature_columns)\n",
    "    )\n",
    "    indicator_columns = list(\n",
    "        map(lambda column: tf.feature_column.indicator_column(column),\n",
    "           categorical_columns)\n",
    "    )\n",
    "    estimator_feature_columns = dense_columns + indicator_columns\n",
    "    print(estimator_feature_columns)\n",
    "    estimator = tf.estimator.DNNRegressor(\n",
    "        feature_columns=estimator_feature_columns,\n",
    "        hidden_units = hparams.hidden_units,\n",
    "        optimizer=tf.train.AdamOptimizer(),\n",
    "        activation_fn=tf.nn.relu,\n",
    "        dropout=hparams.dropout_prob,\n",
    "        config=run_config\n",
    "    )\n",
    "    print(\"\")\n",
    "    print(\"Estimator Type: {}\".format(type(estimator)))\n",
    "    print(\"\")\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Model directory: trained_models/reg-model-01\n",
      "Hyper-paramters: num_epochs=100,batch_size=500,hidden_units=[8, 4],dropout_prob=0.0\n"
     ]
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(\n",
    "    num_epochs = 100,\n",
    "    batch_size = 500,\n",
    "    hidden_units = [8, 4],\n",
    "    dropout_prob = 0.0\n",
    ")\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "run_config = tf.estimator.RunConfig().replace(model_dir=model_dir)\n",
    "print(\"Model directory: {}\".format(run_config.model_dir))\n",
    "print(\"Hyper-paramters: {}\".format(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NumericColumn(key='x', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='y', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='x_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='y_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='dist_xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/reg-model-01', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc0540d4110>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "Estimator Type: <class 'tensorflow_estimator.python.estimator.canned.dnn.DNNRegressor'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator = create_estimator(run_config, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/train-data.csv\n",
      "Dataset size: 12000\n",
      "Batch size: 500\n",
      "Epoch Count: 100\n",
      "Mode: train\n",
      "Thread Count: 1\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "Estimator training started at 02:38:25\n",
      ".......................................\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4271: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4326: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:882: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_models/reg-model-01/model.ckpt.\n",
      "INFO:tensorflow:loss = 155353.92, step = 1\n",
      "INFO:tensorflow:global_step/sec: 361.968\n",
      "INFO:tensorflow:loss = 164910.36, step = 101 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.221\n",
      "INFO:tensorflow:loss = 148996.36, step = 201 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.347\n",
      "INFO:tensorflow:loss = 126962.3, step = 301 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.495\n",
      "INFO:tensorflow:loss = 141646.0, step = 401 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 468.483\n",
      "INFO:tensorflow:loss = 158618.34, step = 501 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.903\n",
      "INFO:tensorflow:loss = 156000.08, step = 601 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.434\n",
      "INFO:tensorflow:loss = 152532.17, step = 701 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 465.609\n",
      "INFO:tensorflow:loss = 127167.28, step = 801 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 456.571\n",
      "INFO:tensorflow:loss = 126829.07, step = 901 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.537\n",
      "INFO:tensorflow:loss = 101179.02, step = 1001 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.249\n",
      "INFO:tensorflow:loss = 101514.56, step = 1101 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.958\n",
      "INFO:tensorflow:loss = 89671.64, step = 1201 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 457.826\n",
      "INFO:tensorflow:loss = 81626.88, step = 1301 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.058\n",
      "INFO:tensorflow:loss = 78252.78, step = 1401 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.754\n",
      "INFO:tensorflow:loss = 63561.234, step = 1501 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.669\n",
      "INFO:tensorflow:loss = 62267.9, step = 1601 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 463.974\n",
      "INFO:tensorflow:loss = 72280.88, step = 1701 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.03\n",
      "INFO:tensorflow:loss = 60008.457, step = 1801 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.159\n",
      "INFO:tensorflow:loss = 69698.23, step = 1901 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.674\n",
      "INFO:tensorflow:loss = 51264.883, step = 2001 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 442.288\n",
      "INFO:tensorflow:loss = 56909.992, step = 2101 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.552\n",
      "INFO:tensorflow:loss = 60036.727, step = 2201 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 468.846\n",
      "INFO:tensorflow:loss = 58823.492, step = 2301 (0.213 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2400 into trained_models/reg-model-01/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 50859.35.\n",
      ".......................................\n",
      "Estimator training finished at 02:38:32\n",
      "\n",
      "Estimator training elapsed time: 6.177756 seconds\n"
     ]
    }
   ],
   "source": [
    "train_input_fn = generate_pandas_input_fn(file_name=TRAIN_DATA_FILE,\n",
    "                                         mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                         num_epochs=hparams.num_epochs,\n",
    "                                         batch_size=hparams.batch_size)\n",
    "if not RESUME_TRAINING:\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow()\n",
    "print(\"Estimator training started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\")\n",
    "\n",
    "estimator.train(input_fn=train_input_fn)\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Estimator training finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Estimator training elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/test-data.csv\n",
      "Dataset size: 5000\n",
      "Batch size: 5000\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2022-01-23T10:39:26Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2022-01-23-10:39:26\n",
      "INFO:tensorflow:Saving dict for global step 2400: average_loss = 121.21656, global_step = 2400, label/mean = 1.0653467, loss = 606082.8, prediction/mean = 0.9807678\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2400: trained_models/reg-model-01/model.ckpt-2400\n",
      "\n",
      "{'average_loss': 121.21656, 'label/mean': 1.0653467, 'loss': 606082.8, 'prediction/mean': 0.9807678, 'global_step': 2400}\n",
      "\n",
      "RMSE: 11.00984\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 5000\n",
    "\n",
    "test_input_fn = generate_pandas_input_fn(file_name=TEST_DATA_FILE, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= TEST_SIZE)\n",
    "\n",
    "results = estimator.evaluate(input_fn=test_input_fn)\n",
    "print(\"\")\n",
    "print(results)\n",
    "rmse = round(math.sqrt(results[\"average_loss\"]),5)\n",
    "print(\"\")\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/test-data.csv\n",
      "Dataset size: 5000\n",
      "Batch size: 5\n",
      "Epoch Count: 1\n",
      "Mode: infer\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Predicted Values: [34.7328, -8.892282, 14.3126135, 5.1370325, -0.11072874]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "predict_input_fn = generate_pandas_input_fn(file_name=TEST_DATA_FILE, \n",
    "                                      mode= tf.estimator.ModeKeys.PREDICT,\n",
    "                                      batch_size= 5)\n",
    "\n",
    "predictions = estimator.predict(input_fn=predict_input_fn)\n",
    "values = list(map(lambda item: item[\"predictions\"][0],list(itertools.islice(predictions, 5))))\n",
    "print()\n",
    "print(\"Predicted Values: {}\".format(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(features):\n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    features[\"xy\"] = tf.multiply(features['x'], features['y'])\n",
    "    features[\"dist_xy\"] = tf.sqrt(tf.squared_difference(features['x'], features['y']))\n",
    "    return features\n",
    "\n",
    "def csv_serving_input_fn():\n",
    "    SERVING_HEADER = ['x', 'y', 'alpha', 'beta']\n",
    "    SERVING_HEADER_DEFAULTS = [[0.0], [0.0], ['NA'], ['NA']]\n",
    "    rows_string_tensor = tf.placeholder(dtype=tf.string,\n",
    "                                       shape=[None],\n",
    "                                       name='csv_rows')\n",
    "    receiver_tensor = {'csv_rows': rows_string_tensor}\n",
    "    row_columns = tf.expand_dims(rows_string_tensor, -1)\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=SERVING_HEADER_DEFAULTS)\n",
    "    features = dict(zip(SERVING_HEADER, columns))\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        features = process_features(features)\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features, receiver_tensor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-9692b1b48d1b>:6: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function has been renamed, use `export_saved_model` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /home/yeyuel/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:95: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['serving_default', 'regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_models/reg-model-01/export/temp-b'1642905585'/saved_model.pbtxt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'trained_models/reg-model-01/export/1642905585'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_dir = model_dir + \"/export\"\n",
    "\n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=csv_serving_input_fn,\n",
    "    as_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "saved_model_dir = export_dir + '/' + os.listdir(path=export_dir)[-1]\n",
    "print(saved_model_dir)\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir=saved_model_dir,\n",
    "    signature_def_key='predict'\n",
    ")\n",
    "start = time.time()\n",
    "output = predictor_fn({'csv_rows': [\"0.5,1,ax01,ax02\", \"-0.5,-1,ax02,bx02\"]})\n",
    "print(\"Elapse: {}ms\".format(time.time() - start))\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
